```markdown
# Process-Mining Prototype

A lightweight end-to-end demo of process-mining and ML-driven case-duration prediction, built with:

- **Django** + **PostgreSQL** (backend & REST API)  
- **PM4Py** for process discovery & analysis  
- **Streamlit** (frontend) driving off the REST API  
- **scikit-learn** & **XGBoost** for ML  
- **Optuna** for hyperparameter tuning  

---

## ğŸ“ Repository Structure

```

process-mining-prototype/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/
â”‚   â””â”€â”€ event\_logs/
â”‚       â””â”€â”€ synthetic\_events.csv  â† generated log
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate\_synthetic\_logs.py
â”‚   â”œâ”€â”€ train\_model.py
â”‚   â””â”€â”€ train\_improved\_model.py
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ manage.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ core/                     â† Django project
â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â””â”€â”€ urls.py
â”‚   â”œâ”€â”€ events/                   â† events app
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ management/commands/load\_events.py
â”‚   â”œâ”€â”€ api/                      â† REST API app
â”‚   â”‚   â”œâ”€â”€ serializers.py
â”‚   â”‚   â”œâ”€â”€ views.py
â”‚   â”‚   â””â”€â”€ urls.py
â”‚   â””â”€â”€ models/                   â† serialized ML artifacts
â”‚       â”œâ”€â”€ case\_duration\_rf.joblib
â”‚       â””â”€â”€ case\_duration\_xgb.joblib
â””â”€â”€ streamlit\_app/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile (optional)

````

---

## ğŸ›  Prerequisites

- **Python 3.10+**  
- **PostgreSQL 15+**  
- **pip** (or use `venv`/`poetry`)  
- **Graphviz** (for PM4Py visualizations)  
- **Node.js** only if you later add a JS frontend  

---

## âš™ï¸ Backend Setup

1. **Create & activate** a virtual environment:
   ```bash
   cd backend
   python3 -m venv .env
   source .env/bin/activate
````

2. **Install dependencies**:

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

3. **Configure database** (in `backend/core/settings.py`):

   ```python
   DATABASES = {
     'default': {
       'ENGINE':   'django.db.backends.postgresql',
       'NAME':     'pm_db',
       'USER':     'pm_user',
       'PASSWORD': 'pm_pass',
       'HOST':     'localhost',
       'PORT':     '5432',
     }
   }
   ```

4. **Apply migrations**:

   ```bash
   python manage.py migrate
   ```

5. **Load or generate event logs**:

   * **Generate** synthetic data:

     ```bash
     chmod +x ../scripts/generate_synthetic_logs.py
     ../scripts/generate_synthetic_logs.py
     ```
   * **Load** into Postgres:

     ```bash
     python manage.py load_events ../data/event_logs/synthetic_events.csv
     ```

6. **Train ML models**:

   * **RandomForest**:

     ```bash
     chmod +x ../scripts/train_model.py
     ../scripts/train_model.py
     ```
   * **XGBoost + Optuna**:

     ```bash
     pip install optuna xgboost
     chmod +x ../scripts/train_improved_model.py
     ../scripts/train_improved_model.py
     ```

7. **Run the server**:

   ```bash
   python manage.py runserver 8000
   ```

---

## ğŸš€ REST API

| Endpoint                               | Description                                      |
| -------------------------------------- | ------------------------------------------------ |
| `GET /api/metrics/`                    | Returns cycle-time metrics & bottleneck list     |
| `GET /api/process-map/`                | Returns an Î±-miner Petri net as PNG              |
| `GET /api/predict-duration/<case_id>/` | Predicts case duration (hours) via trained model |

Test with:

```bash
curl http://localhost:8000/api/metrics/
curl http://localhost:8000/api/process-map/ --output net.png
curl http://localhost:8000/api/predict-duration/CASE_0001/
```

---

## ğŸ“Š Streamlit Dashboard

1. **Switch to the Streamlit env** (you can reuse the backend venv or create a new one):

   ```bash
   cd streamlit_app
   python3 -m venv .env
   source .env/bin/activate
   pip install -r requirements.txt requests
   ```

2. **Configure** the API base URL in `.env` (at project root or in `streamlit_app/`):

   ```dotenv
   API_URL=http://localhost:8000
   ```

3. **Run** the dashboard:

   ```bash
   streamlit run app.py --server.address=0.0.0.0 --server.port=8501
   ```

4. **Features**:

   * **Top-line metrics** & **bottleneck chart** via `/api/metrics/`
   * **Process-map viewer** via `/api/process-map/`
   * **Case-duration predictor** via `/api/predict-duration/â€¦/`

---

## âœ… Whatâ€™s Done

* Data ingestion & storage (Django models + management command)
* Synthetic-log generator & loader (\~25 k events)
* PM4Py utility & shell verification (500 traces)
* REST API for metrics, process-map, predictions
* Streamlit dashboard driving off the API
* Enhanced analytics: multi-miner selection, bottlenecks, throughput charts
* ML pipelines: RandomForest & XGBoost with Optuna tuning

---

## ğŸ”œ Next Steps

* **Dockerization** of backend + frontend + Postgres
* **Authentication** (DRF or JWT) for the API & dashboard
* **Conformance checking** & additional performance metrics
* **Automated retraining** endpoint & CI/CD pipeline
* **Unit/integration tests** and GitHub Actions setup

---

Feel free to open issues or PRsâ€”happy process-mining!
