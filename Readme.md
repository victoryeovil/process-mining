````markdown
# âš™ï¸ Process-Mining Prototype

A demo end-to-end process-mining application, featuring:

- **Django REST API** for:
  - Metrics & bottlenecks  
  - Throughput  
  - Conformance  
  - Process-map visualization  
  - Duration prediction  
  - Model retraining  
- **PostgreSQL** (or SQLite) persistence  
- **Streamlit** frontend for interactive dashboards & â€œUpload & Predict Riskâ€  

---

## ğŸš€ Quickstart with Docker Compose

1. **Clone the repo**  
   ```bash
   git clone https://github.com/your-org/process-mining-prototype.git
   cd process-mining-prototype
````

2. **Create your `.env`**
   Copy the example and edit as needed:

   ```ini
   # .env

   # â€” PostgreSQL (only if using Postgres) â€”
   POSTGRES_DB=pm_db
   POSTGRES_USER=pm_user
   POSTGRES_PASSWORD=pm_pass

   # DATABASE_URL may point at Postgres or SQLite:
   #   Postgres:   postgresql://pm_user:pm_pass@db:5432/pm_db
   #   SQLite:     sqlite:///app/db.sqlite3
   DATABASE_URL=sqlite:///app/db.sqlite3

   # Django settings
   SECRET_KEY=replace-me-with-a-secure-one
   DEBUG=True

   # Frontend â†’ API
   API_URL=http://backend:8000
   ```

3. **Build & start all services**

   ```bash
   docker compose up --build -d
   ```

4. **Verify containers**

   ```bash
   docker compose ps
   ```

5. **Create a Django superuser**

   ```bash
   docker compose exec backend python manage.py createsuperuser
   ```

6. **Load initial data**

   ```bash
   # Using built-in management command:
   docker compose exec backend python manage.py load_events

   # Or from fixture:
   docker compose exec backend python manage.py loaddata synthetic_events
   ```

7. **Train / retrain ML models**

   * **Reopen-risk**

     ```bash
     docker compose exec backend \
       python scripts/train_reopen_classifier.py \
       ./backend/data/event_logs/synthetic_events.csv
     ```
   * **Case-duration**

     ```bash
     docker compose exec backend python scripts/train_improved_model.py
     ```
   * **Or via API** *(admin only)*

     ```bash
     curl -X POST \
       -H "Authorization: Bearer <ADMIN_TOKEN>" \
       http://localhost:8000/api/retrain/
     ```

8. **Browse the apps**

   * **Django API** â†’ `http://localhost:8000/api/metrics/`
     *(login for protected endpoints)*
   * **Streamlit UI** â†’ `http://localhost:8501`

---

## ğŸ› ï¸ Local (no-Docker) Development

> **Prerequisites:** Python 3.10+, `pip`, PostgreSQL (optional; SQLite fallback)

1. **Clone & enter**

   ```bash
   git clone https://github.com/your-org/process-mining-prototype.git
   cd process-mining-prototype
   ```

2. **Create & activate a virtualenv**

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

3. **Install backend deps**

   ```bash
   pip install --upgrade pip
   pip install -r backend/requirements.txt
   ```

4. **Install Streamlit deps**

   ```bash
   pip install -r streamlit_app/requirements.txt
   ```

5. **Configure your `.env`** *(see above)*

6. **Run migrations & create superuser**

   ```bash
   cd backend
   python manage.py migrate
   python manage.py createsuperuser
   ```

7. **Load or generate event data**

   ```bash
   python manage.py load_events
   ```

8. **Train models**

   ```bash
   python scripts/train_reopen_classifier.py ../backend/data/event_logs/synthetic_events.csv
   python scripts/train_improved_model.py
   ```

9. **Start Django**

   ```bash
   python manage.py runserver
   ```

10. **Start Streamlit**

    ```bash
    cd ../streamlit_app
    streamlit run app.py
    ```

---

## ğŸ“‚ Project Layout

```
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ api/               â† DRF views & serializers
â”‚   â”œâ”€â”€ core/              â† Django settings, URLs, wsgi, utils
â”‚   â”œâ”€â”€ data/              â† sample CSV event-logs
â”‚   â”œâ”€â”€ events/            â† `Event` & `Case` models + management commands
â”‚   â”œâ”€â”€ models/            â† persisted ML artifacts (.joblib)
â”‚   â”œâ”€â”€ scripts/           â† training pipelines
â”‚   â”œâ”€â”€ entrypoint.sh      â† Docker entrypoint (migrations + gunicorn)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ manage.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ streamlit_app
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## âš™ï¸ Tips & Troubleshooting

* **Switch DB**: set `DATABASE_URL` to:

  * SQLite: `sqlite:///app/db.sqlite3`
  * Postgres: `postgresql://<user>:<pw>@db:5432/<db>`
* **Inspect volumes**:

  ```bash
  docker run --rm -v process-mining-prototype_db_data:/data busybox ls -R /data
  ```
* **Free up space**:

  ```bash
  docker system prune -af
  docker volume prune -f
  ```
* **View logs**:

  ```bash
  docker compose logs -f backend
  docker compose logs -f streamlit
  ```
* **Rebuild only one service**:

  ```bash
  docker compose build backend
  docker compose up -d backend
  ```

Happy process-mining! ğŸ‰

```
```
