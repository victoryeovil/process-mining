Below is an updated **README.md** for the **process-mining-prototype** repo, with step-by-step instructions for both Docker-based and local (no-Docker) setups, environment variables, migrations, model training, and how to launch the Streamlit UI.

````markdown
# âš™ï¸ Process-Mining Prototype

A demo end-to-end process-mining application, with:

- **Django REST API** for metrics, process-map, conformance, throughput, predictions, and model retraining  
- **PostgreSQL** (or SQLite) persistence  
- **Streamlit** frontend for interactive dashboards and â€œupload & predict riskâ€  

---

## ğŸš€ Quickstart with Docker Compose

1. **Clone the repo**  
   ```bash
   git clone https://github.com/your-org/process-mining-prototype.git
   cd process-mining-prototype
````

2. **Create your `.env` file**
   Copy `â€‹.env.example` to `.env` and edit as needed:

   ```ini
   # .env

   # â€” PostgreSQL (only if using Postgres; see SQLite alternative below) â€”
   POSTGRES_DB=pm_db
   POSTGRES_USER=pm_user
   POSTGRES_PASSWORD=pm_pass

   # DATABASE_URL may point at Postgres or SQLite
   # Use Postgres:      postgresql://pm_user:pm_pass@db:5432/pm_db
   # Or SQLite fallback:
   DATABASE_URL=sqlite:///app/db.sqlite3

   # Backend (Django) settings
   SECRET_KEY=replace-me-with-a-secure-one
   DEBUG=True

   # API URL for Streamlit frontend
   API_URL=http://backend:8000
   ```

3. **Build & run all services**

   ```bash
   docker compose up --build -d
   ```

4. **Check that containers are running**

   ```bash
   docker compose ps
   ```

5. **(First run) Create a Django superuser**

   ```bash
   docker compose exec backend python manage.py createsuperuser
   ```

6. **(If using Postgres) Load initial event data**

   ```bash
   # from project root
   docker compose exec backend python manage.py loaddata synthetic_events
   ```

   Or use your own management command:

   ```bash
   docker compose exec backend python manage.py load_events
   ```

7. **Train or retrain ML models**

   * **Reopen-risk**

     ```bash
     docker compose exec backend python scripts/train_reopen_classifier.py ./backend/data/event_logs/synthetic_events.csv
     ```
   * **Case-duration**

     ```bash
     docker compose exec backend python scripts/train_improved_model.py
     ```

   Or via the REST API (admin only):

   ```bash
   curl -X POST -H "Authorization: Bearer <your_admin_token>" \
     http://localhost:8000/api/retrain/
   ```

8. **Visit the apps**

   * **Django API** â†’ [http://localhost:8000/api/metrics/](http://localhost:8000/api/metrics/)  (youâ€™ll need to log in for protected endpoints)
   * **Streamlit UI** â†’ [http://localhost:8501](http://localhost:8501)

---

## ğŸ› ï¸ Local (no-Docker) Development

> **Prerequisites:** Python 3.10+, `pip`, PostgreSQL (or skip for SQLite),

1. **Clone & enter**

   ```bash
   git clone https://github.com/your-org/process-mining-prototype.git
   cd process-mining-prototype
   ```

2. **Create & activate a virtualenv**

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

3. **Install backend deps**

   ```bash
   pip install --upgrade pip
   pip install -r backend/requirements.txt
   ```

4. **Install Streamlit deps**

   ```bash
   pip install -r streamlit_app/requirements.txt
   ```

5. **Configure your `.env`** (see above).

6. **Run Django migrations**

   ```bash
   cd backend
   python manage.py migrate
   python manage.py createsuperuser
   ```

7. **Load or generate event data**

   ```bash
   python manage.py load_events
   ```

8. **Train models**

   ```bash
   python scripts/train_reopen_classifier.py ../backend/data/event_logs/synthetic_events.csv
   python scripts/train_improved_model.py
   ```

9. **Start the Django server**

   ```bash
   python manage.py runserver
   ```

10. **Run Streamlit** (in repo root)

    ```bash
    cd ../streamlit_app
    streamlit run app.py
    ```

---

## ğŸ“‚ Project Layout

```
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ api/               â† Django REST views & serializers
â”‚   â”œâ”€â”€ core/              â† Django project settings, URLs, wsgi, utils
â”‚   â”œâ”€â”€ data/              â† example CSV event-logs
â”‚   â”œâ”€â”€ events/            â† `Event` & `Case` models, management cmds
â”‚   â”œâ”€â”€ models/            â† persisted ML artifacts (.joblib)
â”‚   â”œâ”€â”€ scripts/           â† training pipelines for ML models
â”‚   â”œâ”€â”€ entrypoint.sh      â† Docker entrypoint (migrations + gunicorn)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ manage.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ streamlit_app
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md  â† you are here
```

---

## âš™ï¸ Tips & Troubleshooting

* **Switching DB**: set `DATABASE_URL` to `sqlite:///app/db.sqlite3` for SQLite, or to a full `postgresql://â€¦` URI for Postgres.
* **Inspecting volumes**:

  ```bash
  docker run --rm -v process-mining-prototype_db_data:/data busybox ls -R /data
  ```
* **Free up space**:

  ```bash
  docker system prune -af
  docker volume prune -f
  ```
* **Logs**:

  ```bash
  docker compose logs -f backend
  docker compose logs -f streamlit
  ```
* **Rebuild only one service**:

  ```bash
  docker compose build backend
  docker compose up -d backend
  ```

---

Happy process-mining! ğŸ‰
